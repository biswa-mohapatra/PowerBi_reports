{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.9.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacremoses) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sacremoses) (4.64.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-22 10:07:33.091675: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.9.1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce the amount of console output from TF\n",
    "import tensorflow as tf\n",
    "!pip install sacremoses\n",
    "from transformers import *\n",
    "!pip install -q datasets # install HF datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import logging\n",
    "\n",
    "print('TF version',tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # check GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tensorflow: setting up strategy\n",
      " XLA Enabled\n",
      " One Device Strategy [CPU] Enabled\n"
     ]
    }
   ],
   "source": [
    "def setup_strategy(xla, fp16, no_cuda):\n",
    "    print(\" Tensorflow: setting up strategy\")\n",
    "    \n",
    "    # setup xla\n",
    "    if xla:\n",
    "        print(\" XLA Enabled\")\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "    \n",
    "    # setup mixed precision training\n",
    "    if fp16:\n",
    "        # Set to float16 at first\n",
    "        print(\" Mixed Precision Training Enabled\")\n",
    "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "    \n",
    "    # setup distribution strategy\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if no_cuda:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    else:\n",
    "        if len(gpus) == 0:\n",
    "            print(\" One Device Strategy [CPU] Enabled\")\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        elif len(gpus) == 1:\n",
    "            print(\" One Device Strategy [GPU] Enabled\")\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        elif len(gpus) > 1:\n",
    "            print(\" Mirrored Strategy Enabled\")\n",
    "            # If only want to use a specific subset of GPUs use CUDA_VISIBLE_DEVICES=0`\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "        else:\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    return strategy\n",
    "\n",
    "def n_replicas(strategy):\n",
    "    # return number of devices\n",
    "    return strategy.num_replicas_in_sync\n",
    "\n",
    "# note: \n",
    "# huggingface TF-T5 implementation has issues when mixed precision is enabled\n",
    "# we will disable FP16 for this but can be used for training any other model\n",
    "strategy = setup_strategy(xla=True, fp16=False, no_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(cache_dir):\n",
    "    # download data using a keras utility\n",
    "    _url = \"https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl\" # download mbpp dataset\n",
    "    dataset_path = tf.keras.utils.get_file(\"mbpp.jsonl\", origin=_url, cache_dir=cache_dir, cache_subdir=cache_dir)\n",
    "    return dataset_path \n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, args):\n",
    "    # encode text-code pairs\n",
    "    texts = examples['text']\n",
    "    codes = examples['code']\n",
    "    # tests = [\" \".join(test) for test in examples['test_list']] # convert list of test cases to single string\n",
    "    \n",
    "    # encode texts by prepending the task for input sequence\n",
    "    inputs = [args.prefix + text for text in texts]\n",
    "    model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # encode texts by prepending the task for input sequence and appending the test sequence\n",
    "    # inputs = [args.prefix + text + \" \" + test for text, test in zip(texts, tests)]\n",
    "    # model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # encode texts by prepending the task for input sequence\n",
    "    labels = tokenizer(codes, max_length=args.max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
    "    \n",
    "    # we need to replace the index of the padding tokens by -100\n",
    "    # such that they are not taken into account by the CrossEntropyLoss\n",
    "    labels_with_ignore_index = []\n",
    "    for labels_example in labels:\n",
    "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
    "        labels_with_ignore_index.append(labels_example)\n",
    "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
    "    \n",
    "    # return features\n",
    "    return model_inputs\n",
    "\n",
    "def get_train_tfdataset(train_dataset, num_train_examples, args):\n",
    "    # select feature columns\n",
    "    columns = ['input_ids', 'attention_mask', 'labels'] \n",
    "    # set to tensorflow format\n",
    "    train_dataset.set_format(type='tensorflow', columns=columns) \n",
    "    \n",
    "    # specify return types\n",
    "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32} \n",
    "    # specify return shapes\n",
    "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])} \n",
    "    # initialize dataset \n",
    "    tf_dataset = tf.data.Dataset.from_generator(lambda : train_dataset, return_types, return_shapes) \n",
    "    \n",
    "    # turn off auto-sharding\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = tf_dataset.with_options(options)\n",
    "    \n",
    "    # repeat, shuffle, batch, prefetch\n",
    "    ds = (\n",
    "        tf_dataset.repeat()\n",
    "        .shuffle(num_train_examples, seed=args.seed)\n",
    "        .batch(args.train_batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    # distribute dataset to devices\n",
    "    return strategy.experimental_distribute_dataset(ds)\n",
    "\n",
    "def get_validation_tfdataset(eval_dataset, num_validation_examples, args):\n",
    "    # select feature columns\n",
    "    columns = ['input_ids', 'attention_mask', 'labels'] \n",
    "    # set to tensorflow format\n",
    "    eval_dataset.set_format(type='tensorflow', columns=columns) \n",
    "    \n",
    "    # specify return types\n",
    "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32} \n",
    "    # specify return shapes\n",
    "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])} \n",
    "    # initialize dataset \n",
    "    tf_dataset = tf.data.Dataset.from_generator(lambda : eval_dataset, return_types, return_shapes) \n",
    "    \n",
    "    # turn off auto-sharding\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    tf_dataset = tf_dataset.with_options(options)\n",
    "    \n",
    "    # repeat, batch, prefetch\n",
    "    ds = (\n",
    "        tf_dataset.repeat()\n",
    "        .batch(args.validation_batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    # distribute dataset to devices\n",
    "    return strategy.experimental_distribute_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    # set random seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
    "    # initialize logger for tracking events and save in file\n",
    "    if isinstance(log_file, Path):\n",
    "        log_file = str(log_file)\n",
    "    log_format = logging.Formatter(\n",
    "        fmt='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "        datefmt='%m/%d/%Y %H:%M:%S'\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(log_format)\n",
    "    logger.handlers = [console_handler]\n",
    "    if log_file and log_file != '':\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(log_file_level)\n",
    "        # file_handler.setFormatter(log_format)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "class ProgressBar(object):\n",
    "    # custom progress bar\n",
    "    def __init__(self, n_total,width=30,desc = 'Training'):\n",
    "        self.width = width\n",
    "        self.n_total = n_total\n",
    "        self.start_time = time.time()\n",
    "        self.desc = desc\n",
    "\n",
    "    def __call__(self, step, info={}):\n",
    "        now = time.time()\n",
    "        current = step + 1\n",
    "        recv_per = current / self.n_total\n",
    "        bar = f'[{self.desc}] {current}/{self.n_total} ['\n",
    "        if recv_per >= 1:\n",
    "            recv_per = 1\n",
    "        prog_width = int(self.width * recv_per)\n",
    "        if prog_width > 0:\n",
    "            bar += '=' * (prog_width - 1)\n",
    "            if current< self.n_total:\n",
    "                bar += \">\"\n",
    "            else:\n",
    "                bar += '='\n",
    "        bar += '.' * (self.width - prog_width)\n",
    "        bar += ']'\n",
    "        show_bar = f\"\\r{bar}\"\n",
    "        time_per_unit = (now - self.start_time) / current\n",
    "        if current < self.n_total:\n",
    "            eta = time_per_unit * (self.n_total - current)\n",
    "            if eta > 3600:\n",
    "                eta_format = ('%d:%02d:%02d' %\n",
    "                              (eta // 3600, (eta % 3600) // 60, eta % 60))\n",
    "            elif eta > 60:\n",
    "                eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
    "            else:\n",
    "                eta_format = '%ds' % eta\n",
    "            time_info = f' - ETA: {eta_format}'\n",
    "        else:\n",
    "            if time_per_unit >= 1:\n",
    "                time_info = f' {time_per_unit:.1f}s/step'\n",
    "            elif time_per_unit >= 1e-3:\n",
    "                time_info = f' {time_per_unit * 1e3:.1f}ms/step'\n",
    "            else:\n",
    "                time_info = f' {time_per_unit * 1e6:.1f}us/step'\n",
    "\n",
    "        show_bar += time_info\n",
    "        if len(info) != 0:\n",
    "            show_info = f'{show_bar} ' + \\\n",
    "                        \"-\".join([f' {key}: {value:.4f} ' if key != \"learning_rate\" else f' {key}: {value:.8f} ' for key, value in info.items()])\n",
    "            print(show_info, end='')\n",
    "        else:\n",
    "            print(show_bar, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, args, train_dataset, validation_dataset, \n",
    "        num_train_examples, num_validation_examples\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.num_train_examples = num_train_examples\n",
    "        \n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.num_validation_examples = num_validation_examples\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.eval_loss = tf.keras.metrics.Sum()\n",
    "        \n",
    "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
    "        # creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.\n",
    "        num_warmup_steps = math.ceil(num_training_steps * self.args.warmup_ratio)\n",
    "        self.optimizer, self.lr_scheduler = create_optimizer(\n",
    "            init_lr=self.args.learning_rate,\n",
    "            num_train_steps=num_training_steps,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            weight_decay_rate=self.args.weight_decay,\n",
    "            adam_epsilon=self.args.adam_epsilon\n",
    "        )\n",
    "    \n",
    "    def evaluation_step(self, features, labels, nb_instances_in_global_batch):\n",
    "        # forward pass\n",
    "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=False)[:2]\n",
    "        loss, logits = outputs[:2]\n",
    "        # loss scaling\n",
    "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
    "        # add current batch loss\n",
    "        self.eval_loss.update_state(scaled_loss)\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_evaluation_steps(self, batch):\n",
    "        features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
    "        labels = batch['labels']\n",
    "        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
    "        # strategy.run() expects args to be a list or tuple\n",
    "        inputs = (features, labels, nb_instances)\n",
    "        # `run` replicates the provided computation and runs with the distributed input\n",
    "        strategy.run(self.evaluation_step, inputs)\n",
    "\n",
    "    def evaluate(self):\n",
    "        # calculate total validation steps\n",
    "        steps = math.ceil(self.num_validation_examples / self.args.validation_batch_size)\n",
    "        # reset eval loss after every epoch\n",
    "        self.eval_loss.reset_states()\n",
    "        logs = {}\n",
    "        pbar = ProgressBar(n_total=steps, desc='Evaluating')\n",
    "        # iterate over validation dataset\n",
    "        for step, batch in enumerate(self.validation_dataset): \n",
    "            # distributed evaluation step\n",
    "            self.distributed_evaluation_steps(batch) \n",
    "            logs[\"eval_loss\"] = self.eval_loss.result() / (step + 1)\n",
    "            pbar(step=step, info=logs)\n",
    "            if step == steps - 1:\n",
    "                break\n",
    "        print(\"\\n------------- validation result -----------------\")\n",
    "        \n",
    "    def apply_gradients(self, features, labels, nb_instances_in_global_batch):\n",
    "        # forward pass\n",
    "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=True)[:2] \n",
    "        loss, logits = outputs[:2]\n",
    "        # loss scaling\n",
    "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype) \n",
    "        # calculate gradients\n",
    "        gradients = tf.gradients(scaled_loss, self.model.trainable_variables) \n",
    "        # convert gradients with nan value\n",
    "        gradients = [g if g is not None else tf.zeros_like(v) for g, v in zip(gradients, self.model.trainable_variables)] \n",
    "        # optimize the model\n",
    "        self.optimizer.apply_gradients(list(zip(gradients, self.model.trainable_variables))) \n",
    "        # add current batch loss\n",
    "        self.train_loss.update_state(scaled_loss) \n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_training_steps(self, batch):\n",
    "        with strategy.scope():\n",
    "            features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
    "            labels = batch['labels']\n",
    "            nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
    "            # strategy.run() expects args to be a list or tuple\n",
    "            inputs = (features, labels, nb_instances)\n",
    "            # `run` replicates the provided computation and runs with the distributed input.\n",
    "            strategy.run(self.apply_gradients, inputs)\n",
    "    \n",
    "    def train(self):\n",
    "        # calculate total training steps\n",
    "        num_updates_per_epoch = self.num_train_examples // args.train_batch_size \n",
    "        self.steps_per_epoch = num_updates_per_epoch\n",
    "        t_total = self.steps_per_epoch * self.args.epochs\n",
    "        \n",
    "        with strategy.scope():\n",
    "            # optimizer, and checkpoint must be created under `strategy.scope`\n",
    "            # create optimizer and scheduler\n",
    "            self.create_optimizer_and_scheduler(num_training_steps=t_total) \n",
    "            \n",
    "            # create checkpoint manager\n",
    "            folder = os.path.join(self.args.output_dir, self.args.checkpoint_dir)\n",
    "            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model) \n",
    "            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
    "            iterations = self.optimizer.iterations\n",
    "            \n",
    "            logger.info(\"***** Running training *****\")\n",
    "            logger.info(f\"  Num examples = {self.num_train_examples}\")\n",
    "            logger.info(f\"  Num Epochs = {self.args.epochs}\")\n",
    "            logger.info(f\"  Total train batch size (w. parallel & distributed) = {self.args.train_batch_size * n_replicas(strategy)}\")\n",
    "            logger.info(f\"  Steps per epoch = {self.steps_per_epoch}\")\n",
    "            logger.info(f\"  Total optimization steps = {t_total}\")\n",
    "            \n",
    "            self.train_loss = tf.keras.metrics.Sum(name=\"training_loss\")\n",
    "            start_time = datetime.datetime.now()\n",
    "            for epoch_iter in range(self.args.epochs):\n",
    "                # training loop\n",
    "                logger.info(f\"Epoch {epoch_iter + 1}/{self.args.epochs}\")\n",
    "                \n",
    "                pbar = ProgressBar(n_total=self.steps_per_epoch, desc='Training')\n",
    "                # iterate over training dataset\n",
    "                for step, batch in enumerate(self.train_dataset):    \n",
    "                    # distributed training step\n",
    "                    self.distributed_training_steps(batch) \n",
    "                    \n",
    "                    self.global_step = iterations.numpy()\n",
    "                    training_loss = self.train_loss.result() / (step + 1)\n",
    "                    \n",
    "                    logs = {}\n",
    "                    logs[\"training_loss\"] = training_loss.numpy()\n",
    "                    logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n",
    "                    pbar(step=step, info=logs)\n",
    "                    \n",
    "                    if self.global_step % self.steps_per_epoch == 0:\n",
    "                        print(\"\\n------------- train result -----------------\")\n",
    "                        # call to evaluation loop\n",
    "                        self.evaluate()\n",
    "                        # save checkpoint\n",
    "                        ckpt_save_path = self.model.ckpt_manager.save()\n",
    "                        logger.info(f\"Saving checkpoint at {ckpt_save_path}\")\n",
    "                        break\n",
    "                \n",
    "                # reset train loss after every epoch\n",
    "                self.train_loss.reset_states()\n",
    "            end_time = datetime.datetime.now()\n",
    "            logger.info(f\"Training took: {str(end_time - start_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cache_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    logger.info(\" Starting training / evaluation\")\n",
    "    \n",
    "    logger.info(\" Downloading Data Files\")\n",
    "    dataset_path = download_dataset(args.cache_dir) \n",
    "\n",
    "    logger.info(\" Loading Data Files\")\n",
    "    dataset = load_dataset('json', data_files=dataset_path) \n",
    "    # train test split\n",
    "    dataset = dataset['train'].train_test_split(0.1, shuffle=False) \n",
    "        \n",
    "    logger.info(\" Initializing Tokenizer\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name) \n",
    "    \n",
    "    logger.info(\" Preparing Features\")\n",
    "    dataset = dataset.map(convert_examples_to_features, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"args\":args})\n",
    "\n",
    "    logger.info(\" Intializing training and validation dataset \")\n",
    "    train_dataset = dataset['train']\n",
    "    num_train_examples = len(dataset['train'])\n",
    "    # create tf train dataset\n",
    "    tf_train_dataset = get_train_tfdataset(train_dataset, num_train_examples, args) \n",
    "    \n",
    "    validation_dataset = dataset['test']\n",
    "    num_validation_examples = len(dataset['test'])\n",
    "    # create tf validation dataset\n",
    "    tf_validation_dataset = get_validation_tfdataset(train_dataset, num_validation_examples, args) \n",
    "    \n",
    "    logger.info(f' Intializing model | {args.model_type.upper()} ')\n",
    "    with strategy.scope():\n",
    "        # model must be created under `strategy.scope`\n",
    "        model = TFT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, from_pt=True)\n",
    "    \n",
    "    # custom training loop\n",
    "    trainer = Trainer(model, args, tf_train_dataset, tf_validation_dataset, num_train_examples, num_validation_examples) \n",
    "    trainer.train()\n",
    "    \n",
    "    # save pretrained model and tokenizer\n",
    "    logger.info(f\" Saving model in {args.save_dir}\")\n",
    "    trainer.model.save_pretrained(args.save_dir)\n",
    "    tokenizer.save_pretrained(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:13:39 - INFO - root -    Starting training / evaluation\n",
      "07/22/2022 11:13:39 - INFO - root -    Downloading Data Files\n",
      "07/22/2022 11:13:39 - INFO - root -    Loading Data Files\n",
      "07/22/2022 11:13:40 - WARNING - datasets.builder -   Using custom data configuration default-2b22f9d303ce4bf4\n",
      "07/22/2022 11:13:40 - WARNING - datasets.builder -   Reusing dataset json (/home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "100%|██████████| 1/1 [00:00<00:00, 411.49it/s]\n",
      "07/22/2022 11:13:40 - INFO - root -    Initializing Tokenizer\n",
      "07/22/2022 11:13:41 - INFO - root -    Preparing Features\n",
      "07/22/2022 11:13:42 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-49f605418818a00b.arrow\n",
      "07/22/2022 11:13:42 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5/cache-8bb6f14a714e9fdc.arrow\n",
      "07/22/2022 11:13:42 - INFO - root -    Intializing training and validation dataset \n",
      "07/22/2022 11:13:42 - INFO - root -    Intializing model | T5 \n",
      "07/22/2022 11:13:46 - INFO - root -   ***** Running training *****\n",
      "07/22/2022 11:13:46 - INFO - root -     Num examples = 876\n",
      "07/22/2022 11:13:46 - INFO - root -     Num Epochs = 5\n",
      "07/22/2022 11:13:46 - INFO - root -     Total train batch size (w. parallel & distributed) = 8\n",
      "07/22/2022 11:13:46 - INFO - root -     Steps per epoch = 109\n",
      "07/22/2022 11:13:46 - INFO - root -     Total optimization steps = 545\n",
      "07/22/2022 11:13:46 - INFO - root -   Epoch 1/5\n",
      "2022-07-22 11:13:46.805861: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 109/109 [==============================] 2.4s/step  training_loss: 2.7355 - learning_rate: 0.00030000  \n",
      "------------- train result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:18:04.134162: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluating] 13/13 [==============================] 949.8ms/step  eval_loss: 1.2853 \n",
      "------------- validation result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:18:19 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-1\n",
      "07/22/2022 11:18:19 - INFO - root -   Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 109/109 [==============================] 2.1s/step  training_loss: 1.2473 - learning_rate: 0.00022500  \n",
      "------------- train result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:22:13.663223: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluating] 13/13 [==============================] 690.8ms/step  eval_loss: 0.8313 \n",
      "------------- validation result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:22:25 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-2\n",
      "07/22/2022 11:22:25 - INFO - root -   Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 109/109 [==============================] 2.2s/step  training_loss: 0.9159 - learning_rate: 0.00015000  \n",
      "------------- train result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:26:21.901696: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluating] 13/13 [==============================] 728.7ms/step  eval_loss: 0.5386 \n",
      "------------- validation result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:26:34 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-3\n",
      "07/22/2022 11:26:34 - INFO - root -   Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 109/109 [==============================] 2.2s/step  training_loss: 0.6705 - learning_rate: 0.00007500  \n",
      "------------- train result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:30:29.647781: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluating] 13/13 [==============================] 682.7ms/step  eval_loss: 0.3476 \n",
      "------------- validation result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:30:41 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-4\n",
      "07/22/2022 11:30:41 - INFO - root -   Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 109/109 [==============================] 2.2s/step  training_loss: 0.5200 - learning_rate: 0.00000000  \n",
      "------------- train result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 11:34:36.438867: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluating] 13/13 [==============================] 694.6ms/step  eval_loss: 0.2702 \n",
      "------------- validation result -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:34:48 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-5\n",
      "07/22/2022 11:34:48 - INFO - root -   Training took: 0:21:01.563153\n",
      "07/22/2022 11:34:48 - INFO - root -    Saving model in runs//saved_model/\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    # define training arguments\n",
    "    \n",
    "    # MODEL\n",
    "    model_type = 't5'\n",
    "    tokenizer_name = 'Salesforce/codet5-base'\n",
    "    model_name_or_path = 'Salesforce/codet5-base'\n",
    "    \n",
    "    # DATA\n",
    "    train_batch_size = 8\n",
    "    validation_batch_size = 8\n",
    "    max_input_length = 48\n",
    "    max_target_length = 128\n",
    "    prefix = \"Generate Python: \"    \n",
    "\n",
    "    # OPTIMIZER\n",
    "    learning_rate = 3e-4\n",
    "    weight_decay = 1e-4\n",
    "    warmup_ratio = 0.2\n",
    "    adam_epsilon = 1e-8\n",
    "\n",
    "    # TRAINING\n",
    "    seed = 2022\n",
    "    epochs = 5\n",
    "\n",
    "    # DIRECTORIES\n",
    "    output_dir = \"runs/\"\n",
    "    logging_dir = f\"{output_dir}/logs/\"\n",
    "    checkpoint_dir = f\"checkpoint\"\n",
    "    save_dir = f\"{output_dir}/saved_model/\"\n",
    "    cache_dir = './'\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "# initialize training arguments\n",
    "args = Args()\n",
    "# initialize logger\n",
    "logger = init_logger(log_file=os.path.join(args.logging_dir, f\"{args.model_type}-{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())}.log\"))\n",
    "# fix all seeds\n",
    "fix_all_seeds(args.seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run training and evaluation\n",
    "    dataset = run(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(args, text):\n",
    "    # load saved finetuned model\n",
    "    model = TFT5ForConditionalGeneration.from_pretrained(args.save_dir)\n",
    "    # load saved tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(args.save_dir) \n",
    "    \n",
    "     # encode texts by prepending the task for input sequence and appending the test sequence\n",
    "    query = args.prefix + text \n",
    "    encoded_text = tokenizer(query, return_tensors='tf', padding='max_length', truncation=True, max_length=args.max_input_length)\n",
    "    \n",
    "    # inference\n",
    "    generated_code = model.generate(\n",
    "        encoded_text[\"input_ids\"], attention_mask=encoded_text[\"attention_mask\"], \n",
    "        max_length=args.max_target_length, top_p=0.95, top_k=50, repetition_penalty=2.0, num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    # decode generated tokens\n",
    "    decoded_code = tokenizer.decode(generated_code.numpy()[0], skip_special_tokens=True)\n",
    "    return decoded_code\n",
    "\n",
    "def predict_from_dataset(args):\n",
    "    # load using hf datasets\n",
    "    dataset = load_dataset('json', data_files='./mbpp.jsonl') \n",
    "    # train test split\n",
    "    dataset = dataset['train'].train_test_split(0.1, shuffle=False) \n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    # randomly select an index from the validation dataset\n",
    "    index = random.randint(0, len(test_dataset))\n",
    "    text = test_dataset[index]['text']\n",
    "    code = test_dataset[index]['code']\n",
    "    \n",
    "    # run-predict on text\n",
    "    decoded_code = run_predict(args, text)\n",
    "    \n",
    "    print(\"#\" * 25); print(\"QUERY: \", text); \n",
    "    print()\n",
    "    print('#' * 25); print(\"ORIGINAL: \"); print(\"\\n\", code);\n",
    "    print()\n",
    "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n",
    "    \n",
    "def predict_from_text(args, text):\n",
    "    # run-predict on text\n",
    "    decoded_code = run_predict(args, text)\n",
    "    print(\"#\" * 25); print(\"QUERY: \", text); \n",
    "    print()\n",
    "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:36:40 - WARNING - datasets.builder -   Using custom data configuration default-2b22f9d303ce4bf4\n",
      "07/22/2022 11:36:40 - WARNING - datasets.builder -   Reusing dataset json (/home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "100%|██████████| 1/1 [00:00<00:00, 518.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "QUERY:  Write a function to find the maximum of similar indices in two lists of tuples.\n",
      "\n",
      "#########################\n",
      "ORIGINAL: \n",
      "\n",
      " def max_similar_indices(test_list1, test_list2):\n",
      "  res = [(max(x[0], y[0]), max(x[1], y[1]))\n",
      "   for x, y in zip(test_list1, test_list2)]\n",
      "  return (res) \n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " def max_similarity(testlist1, testList2):\n",
      "  res = min([bisect for i in zip(*map (sorted, tup))] ) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:36:53 - WARNING - datasets.builder -   Using custom data configuration default-2b22f9d303ce4bf4\n",
      "07/22/2022 11:36:53 - WARNING - datasets.builder -   Reusing dataset json (/home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "100%|██████████| 1/1 [00:00<00:00, 614.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "QUERY:  Write a python function to check whether all the bits are within a given range or not.\n",
      "\n",
      "#########################\n",
      "ORIGINAL: \n",
      "\n",
      " def all_Bits_Set_In_The_Given_Range(n,l,r): \n",
      "    num = ((1 << r) - 1) ^ ((1 << (l - 1)) - 1) \n",
      "    new_num = n & num \n",
      "    if (num == new_num): \n",
      "        return True\n",
      "    return False\n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " def all_BitsAreWithin(n,m): \n",
      "    if (max - min) > n:\n",
      "        return False  \n",
      "      res = True; temp=0 ; count1 += 1      \n",
      "    while((temp + arr[count2]) % m == 0);    \n",
      "          for i in range(-min+len([i])) / 2.5 or ((res | max)) >= a ) :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/22/2022 11:37:19 - WARNING - datasets.builder -   Using custom data configuration default-2b22f9d303ce4bf4\n",
      "07/22/2022 11:37:19 - WARNING - datasets.builder -   Reusing dataset json (/home/jupyter/.cache/huggingface/datasets/json/default-2b22f9d303ce4bf4/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n",
      "100%|██████████| 1/1 [00:00<00:00, 536.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "QUERY:  Write a function to combine two given sorted lists using heapq module.\n",
      "\n",
      "#########################\n",
      "ORIGINAL: \n",
      "\n",
      " from heapq import merge\n",
      "def combine_lists(num1,num2):\n",
      "  combine_lists=list(merge(num1, num2))\n",
      "  return combine_lists\n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " import heapq as hQ\n",
      "def combine_sorted(nums1, nums2): \n",
      "    result = [h q.merge(*map((x, y) for x in zip([y], i)))]  \n",
      "        return list (result).sort()\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "predict_from_dataset(args)\n",
    "# example 2\n",
    "predict_from_dataset(args)\n",
    "# example 3\n",
    "predict_from_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "QUERY:  Write a function to add two random numbers\n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " def add_random(n,m):\n",
      "    if m >= n: \n",
      "        return False  \n",
      "      num1 = int((i + 1) * (2 ** len($v)) / 2);    \n",
      "\tnum0 += a;      \n",
      "\n",
      "#########################\n",
      "QUERY:  Write a function to find the frequency of items in a list\n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " from collections import Counter\n",
      "def freq_count(list1): \n",
      "    result = {}  \n",
      "        for item in list:    \n",
      "            if len (item) == 0 or count % 2!= 1():      res.setdefault(\"items\",[]).append((key, value))      \n",
      "                returnresult\n",
      "\n",
      "#########################\n",
      "QUERY:  Write a function to concatenate two dictionary\n",
      "\n",
      "#########################\n",
      "GENERATED: \n",
      "\n",
      " def concatenate_dict(d1, d2):\n",
      "    result = {k: v for k in zip(*map([str(), dict.fromkeys()), str('+', i))} \n",
      "        return list((result)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "predict_from_text(args, \"Write a function to add two random numbers\"); print()\n",
    "# example 2\n",
    "predict_from_text(args, \"Write a function to find the frequency of items in a list\"); print()\n",
    "# example 3\n",
    "predict_from_text(args, \"Write a function to concatenate two dictionary\"); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
